{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ClassificaÃ§Ã£o de Imagens de GrÃ£os de FeijÃ£o ğŸ«˜\n",
    "#### O classificador tem as seguientes etapas\n",
    "1Âª Primeiro: realizamos o treinamento do modelo convolucional utilizando o TensorFlow, com o objetivo de distinguir entre feijÃµes de qualidade normal (1) e estragados (0).\n",
    "\n",
    "\n",
    "2Âª Para avaliar a eficÃ¡cia do modelo: utilizamos imagens contendo uma variedade de feijÃµes. Empregamos tÃ©cnicas de limiarizaÃ§Ã£o e identificaÃ§Ã£o de contornos para individualizar cada feijÃ£o na imagem, sobre os quais aplicamos o modelo de classificaÃ§Ã£o previamente treinado para determinar sua categoria.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento\n",
    "\n",
    "- O cÃ³digo comeÃ§a definindo o caminho para a pasta que contÃ©m as imagens a serem usadas no treinamento e o caminho para o arquivo xlsx que contÃ©m as classes associadas a cada imagem.\n",
    "\n",
    "- A funÃ§Ã£o carregar_imagens Ã© definida para ler e redimensionar as imagens da pasta especificada. Ela itera sobre todos os arquivos na pasta, lÃª cada imagem usando OpenCV, redimensiona-as para o tamanho especificado e as armazena em um dicionÃ¡rio, onde as chaves sÃ£o os nomes dos arquivos e os valores sÃ£o as imagens redimensionadas.\n",
    "\n",
    "- ConcatenaÃ§Ã£o das classes com as imagens, as rÃ³tulos de classe sÃ£o lidos do arquivo xlsx e associados a cada imagem com base no nome do arquivo. As imagens e seus rÃ³tulos correspondentes sÃ£o armazenados em duas listas separadas.\n",
    "\n",
    "- O modelo Ã© construÃ­do utilizando camadas convolucionais para extrair caracterÃ­sticas das imagens, seguidas por camadas de pooling para reduzir a dimensionalidade e camadas densas para processar essas caracterÃ­sticas.ApÃ³s a construÃ§Ã£o do modelo, ele Ã© compilado usando o otimizador 'adam', que Ã© uma tÃ©cnica de otimizaÃ§Ã£o popular para treinar redes neurais, a funÃ§Ã£o de perda 'binary_crossentropy', escolhida devido ao problema de classificaÃ§Ã£o binÃ¡ria, e a mÃ©trica de 'accuracy', que mede a precisÃ£o do modelo durante o treinamento.\n",
    "\n",
    "- O treinamento do modelo Ã© realizado utilizando o mÃ©todo fit, onde as imagens de treinamento e seus respectivos rÃ³tulos sÃ£o fornecidos. O nÃºmero de Ã©pocas Ã© definido como 20, o que significa que o conjunto de dados completo serÃ¡ passado pela rede 20 vezes durante o treinamento. Durante este processo, o modelo ajusta seus pesos iterativamente para minimizar a funÃ§Ã£o de perda, otimizando assim sua capacidade de realizar previsÃµes precisas, por fim o modelo Ã© salvo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sensix\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 153ms/step - accuracy: 0.4649 - loss: 260.5984\n",
      "Epoch 2/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 147ms/step - accuracy: 0.4768 - loss: 21.2807\n",
      "Epoch 3/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 154ms/step - accuracy: 0.5276 - loss: 1.1145\n",
      "Epoch 4/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 143ms/step - accuracy: 0.6940 - loss: 0.5693\n",
      "Epoch 5/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 148ms/step - accuracy: 0.9384 - loss: 0.3688\n",
      "Epoch 6/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 150ms/step - accuracy: 0.8075 - loss: 0.3634\n",
      "Epoch 7/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 158ms/step - accuracy: 0.9185 - loss: 0.2152\n",
      "Epoch 8/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 150ms/step - accuracy: 0.9060 - loss: 0.2101\n",
      "Epoch 9/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 153ms/step - accuracy: 0.9629 - loss: 0.1072\n",
      "Epoch 10/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 152ms/step - accuracy: 0.9563 - loss: 0.0928\n",
      "Epoch 11/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 151ms/step - accuracy: 0.9416 - loss: 0.0986\n",
      "Epoch 12/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 151ms/step - accuracy: 0.9389 - loss: 0.1645\n",
      "Epoch 13/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 148ms/step - accuracy: 0.9715 - loss: 0.0856\n",
      "Epoch 14/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 150ms/step - accuracy: 0.9870 - loss: 0.0613\n",
      "Epoch 15/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 158ms/step - accuracy: 0.9914 - loss: 0.0521\n",
      "Epoch 16/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 149ms/step - accuracy: 0.9800 - loss: 0.0702\n",
      "Epoch 17/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 181ms/step - accuracy: 0.9883 - loss: 0.0533\n",
      "Epoch 18/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 159ms/step - accuracy: 0.9947 - loss: 0.0291\n",
      "Epoch 19/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 141ms/step - accuracy: 1.0000 - loss: 0.0149\n",
      "Epoch 20/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 142ms/step - accuracy: 0.9893 - loss: 0.0307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# caminho para a pasta de imagens e o arquivo da planilha com as classes\n",
    "pasta_imagens = r\"C:\\Users\\sensix\\Desktop\\PESSOAL\\PESSOAL\\SCRIPTS\\feijoes\\classificacao_feijoes\\recortados\"\n",
    "arquivo_planilha = r\"C:\\Users\\sensix\\Desktop\\PESSOAL\\PESSOAL\\SCRIPTS\\feijoes\\classificacao_feijoes\\nomes_das_imagens.xlsx\"\n",
    "\n",
    "# FunÃ§Ã£o para carregar e redimensionar as imagens da pasta\n",
    "def carregar_imagens(pasta, tamanho):\n",
    "    imagens = {}\n",
    "    for filename in os.listdir(pasta):\n",
    "        img = cv2.imread(os.path.join(pasta, filename))\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, tamanho) \n",
    "            imagens[filename] = img\n",
    "    return imagens\n",
    "\n",
    "tamanho_imagem = (128, 128)\n",
    "\n",
    "imagens_dict = carregar_imagens(pasta_imagens, tamanho_imagem)\n",
    "\n",
    "# Converter as imagens para matrizes e normalizaÃ§Ã£o\n",
    "imagens = np.array(list(imagens_dict.values())) / 255.0\n",
    "\n",
    "# ConcatenaÃ§Ã£o das classes com as imagens\n",
    "planilha = pd.read_excel(arquivo_planilha)\n",
    "rotulos_dict = dict(zip(planilha['nome'], planilha['classe']))\n",
    "\n",
    "imagens_ordem = []\n",
    "rotulos_ordem = []\n",
    "for filename, img in imagens_dict.items():\n",
    "    imagens_ordem.append(img)\n",
    "    rotulos_ordem.append(rotulos_dict[filename])\n",
    "\n",
    "X = np.array(imagens_ordem)\n",
    "y = np.array(rotulos_ordem)\n",
    "\n",
    "# ConstruÃ§Ã£o do Modelo\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compilar o modelo\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Treinamento do Modelo \n",
    "history = model.fit(X, y, epochs=20)\n",
    "\n",
    "# Salvar o modelo treinado\n",
    "model.save('modelo_feijoes.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SegmentaÃ§Ã£o da Imagem de Teste e ClassificaÃ§Ã£o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A parte final do cÃ³digo inicia carregando uma imagem contendo mÃºltiplos feijÃµes convertendo para escala de cinza. Em seguida, aplica-se uma tÃ©cnica de limiarizaÃ§Ã£o para destacar os feijÃµes do fundo, seguida por uma operaÃ§Ã£o morfolÃ³gica de fechamento para suavizar as bordas dos objetos. \n",
    "\n",
    "- Os contornos dos feijÃµes sÃ£o identificados e iterados. Para cada feijÃ£o, sua regiÃ£o Ã© recortada e preparada para classificaÃ§Ã£o. ApÃ³s a previsÃ£o do modelo, retÃ¢ngulos sÃ£o desenhados na imagem original, com cores distintas indicando a saÃºde do grÃ£o, saudÃ¡veis (verde) ou estragados (vermelho). Por fim, a imagem classificada Ã© salva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carregar a imagem com vÃ¡rios feijÃµes\n",
    "imagem_original = cv2.imread(r\"C:\\Users\\sensix\\Desktop\\PESSOAL\\PESSOAL\\SCRIPTS\\feijoes\\classificacao_feijoes\\Amostra_normal.jpg\")\n",
    "imagem_gray = cv2.cvtColor(imagem_original, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# LimiarizaÃ§Ã£o da imagem\n",
    "thresh = cv2.threshold(imagem_gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "\n",
    "# Aplicar operaÃ§Ã£o morfolÃ³gica de fechamento\n",
    "kernel = np.ones((5,5), np.uint8)  \n",
    "imagem_abertura = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "# Encontrar contornos dos feijÃµes\n",
    "contornos, _ = cv2.findContours(imagem_abertura, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Iterar sobre os contornos\n",
    "for cnt in contornos:\n",
    "    # Calcular a caixa delimitadora de cada feijÃ£o\n",
    "    x, y, w, h = cv2.boundingRect(cnt)\n",
    "    \n",
    "    # Recortar o feijÃ£o da imagem original\n",
    "    feijao_recortado = imagem_gray[y:y+h, x:x+w]\n",
    "    \n",
    "    # Converter a imagem recortada para uma imagem colorida\n",
    "    feijao_recortado = cv2.merge([feijao_recortado]*3)\n",
    "    \n",
    "    # Redimensionar o feijÃ£o recortado para o tamanho esperado pelo modelo e adicionar a dimensÃ£o do lote\n",
    "    feijao_recortado = cv2.resize(feijao_recortado, (128, 128))  \n",
    "    feijao_recortado = np.expand_dims(feijao_recortado, axis=0)  \n",
    "\n",
    "    previsao = model.predict(feijao_recortado)\n",
    "    \n",
    "    # Definir a cor do retÃ¢ngulo com base na previsÃ£o\n",
    "    cor = (0, 255, 0)  # verde por padrÃ£o (normal)\n",
    "    if previsao[0][0] < 0.5:  # se a previsÃ£o for inferior a 0.5, o feijÃ£o estÃ¡ estragado\n",
    "        cor = (0, 0, 255)  # vermelho (estragado)\n",
    "    \n",
    "    # Desenhar o retÃ¢ngulo delimitador na imagem original\n",
    "    cv2.rectangle(imagem_original, (x, y), (x+w, y+h), cor, 2)\n",
    "\n",
    "# Salvar a imagem classificada\n",
    "cv2.imwrite('feijoes_classificados.png', imagem_original)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
